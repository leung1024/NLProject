

# 实现目标
1. 核心：格式化, 量化
1. 新闻信息量化
	- 提取新闻特征信息，并存入数据库中
	- 利用特征信息，计算某类新闻与某些期货产品的相关度
	- 计算新闻提供者对期货或者股票价格的影响力
	- 股票的新闻热度
	- 用户能够通调用接口，获取数据库的信息或者新闻的特征信息
1. 输入某个股票或者期货，能得到新闻热度、相关程度等信息

# 阶段目标
1. 完成关键新闻的提取
    - 完成数据接口的设计
    - 实现对新闻语料进行预处理
    - 完成特征提取模块，并将相关的信息打印在命令行种
    - 完成数据库设计，并将特征数据存入数据库中
    - 利用DAG图定义新闻处理逻辑
1. 句意分析
1. 新闻热度分析
1. 分析期货与新闻类别的相关性

# 系统架构图
![](http://ww1.sinaimg.cn/large/681b503dgy1fqymtbqkg6j20pf0hxgmk.jpg)

# 技术选型
- 数据存储 -- > MongoDB
    - 因为新闻特征信息的个数随时有可能改变，数据表的结构有变动的可能性，所以这里选用MongoDB作为数据库存储
- RPC框架 -- > Thrift
    - 功能比较完善
    - 企业背书：美团、饿了么
    - 可以实现多种编程语言的调用，而且通信是基于RPC进行的。
    - 序列化和反序列化效率比较高（待测试）

# 数据库设计
- 期货新闻特征数据库

    | 字段                | 类型   | 描述                     | 备注                     |
    | ------------------- | ------ | ------------------------ | ------------------------ |
    | _id                 | int    | 新闻的唯一标示符         |                          |
    | title               | str    | 新闻标题                 |                          |
    | post_date           | str    | 新闻发布日期             |                          |
    | news_source            | str   | 新闻来源 |
    | key_sentencens      | list   | 新闻中需要关注的句子     |                          |
    | relate_future_name  | list   | 相关的期货名称           |
    | relate_contract_ids | list   | 与该新闻相关的期货id     |
    | information_category    | str   | 信息类别               | 股票，期货等|
    | keywords | list | 新闻关键词||
    | news_event | list | 新闻所描述的事件 | 手续费，交易限额|
    | news_source         | str    | 新闻的来源               |                          |

# 模块设计
1. 数据获取模块
    -  输入：查询日期，股票代码，股票名称
    -  输出：符合查询条件的新闻
2. 文字预处理模块
    -  输入：新闻内容
    -  输出：预处理和分词的结果
    -  备注：
        -  需要额外维护一个自定义字典（股票名称，股票代码等）
3. 文本信息提取模块
    - 输入：分词信息
    - 输出：
        - 关键词频统计,将新闻归类
        - 保存某期货商品或股票在近期某些费用的净浮动值
    - 备注：
        - 关注点：交易手续费的改变，交割手续费的改变，交易限额，交割仓库的库容
4. 外部接口模块
	- 股票相关度计算模块
	- 股票热度计算模块

# 代码部分
## 项目文件结构图
```shell
NewsNLP/
|---requirements.txt
|---.gitignore
|---README.md
    source/ # --------------------------------- 源代码
    |---main.py
    |---TrainModel.py
    |---WordsVector.py
    |---dag_main.py
    |---tools.py
        data/
        |---dataAPI.py
        |---MongoConnector.py
        |---__init__.py
        |---DBFactory.py
        DAGFlow/
        |---__init__.py
        |---DAG.py
        |---main.py
            Operator/
            |---TextOperator.py
            |---__init__.py
            scripts/
            |---__init__.py
            |---demo.py
        models/
        |---TextProcess.py
        |---MatchText.py
        |---__init__.py
        |---TextProcessFactory.py
    words_dict/ # ----------------------------- 存放各种词典
    |---stop_words.txt
    |---user_dict
    |---common_keywords_dict.txt
        keywords/
        |---common_keywords_dict.txt
            future/
            |---future_keywords_dict.txt
            |---dce_keywords_dict.txt
            |---shfe_keywords_dict.txt
        word_translate/
        |---common_future_name.txt
            future/
            |---shfe_future_name.txt
            |---common_future_name.txt
            |---dce_future_name.txt
    doc/ # ------------------------------------ 文档部分
    |---DAG流模块.md
    |---项目文档.md
        image/
        |---计算流图demo.png
```
## 类的设计
### 新闻和特征数据接口设计
![](image/数据接口设计.png)

### 文本类设计
- Text Class
![](http://ww1.sinaimg.cn/large/681b503dgy1fr6pjsa5kcj20u10iuaar.jpg)

## 模块
### 数据获取模块
```python
class NewsData(object):
    '''
    该类主定义了新闻数据的获取和保存
    '''
    def __init__(self,
                collection_name=None,
                collection_pattern=None):
        pass

    # 从mongodb中获取新闻数据
    def get_from_mongodb(self, date_range=None, **kwargs):
        pass

    # 从文件中获取新闻数据
    def get_from_file(self):
        pass

    # 新闻特征保存
    def save_news_features_to_mongodb(self, data):
        pass

class DBFactory(object):
    '''
    用于创建mongoDB的相关对象
    '''
    @staticmethod
    # 创建mongoDB相关的对象
    def create_mongo_obj(db_name, db_host='localhost', db_port=27017):
        if db_name == 'webcrawler':
            return MongoWebCrawlerConnector(db_host, db_port)
        elif db_name == 'news_feature':
            return MongoNewsFeatureConnector(db_host, db_port)
        else:
            raise Exception("cannot found %s in factory class" % db_name)

class MongoWebCrawlerConnector(object):
    '''
    定义Crawler数据库的存储接口
    '''
    def __init__(self, host='localhost', port=27017):
        pass

    # 寻找多个
    def find_many(self, collection, **kwargs):
        pass

    # 根据时间段寻找
    def find_by_date(self, collection, d_start, d_end, **kwargs):
        pass
    
    # 获取所有的collections
    def get_collection_list(self, pattern=None):
        pass
    
    # 保存数据
    def save_data(self, collection, data):
        pass

class MongoNewsFeatureConnector(object):
    '''
    定义NewsFeature数据库的存储接口
    '''
    def __init__(self, host='localhost', port=27017):
        pass

    def get_collection_list(self, pattern=None):
        pass
    
    # 参数检查
    def check_data_type(self, data):
        pass

    def save_data(self, collection, data, skip_exist=False):
        pass

    def update_data(self, collection, data):
        pass
```

### 文本预处理模块

### 文本关键信息提取模块

## 运行样例
### 普通模式
``` python
#coding=utf-8
from pprint import pprint
from data.dataAPI import NewsData
from WordsVector import WordVector
from models.TextProcess import SHFEFutureText
from models.MatchText import MatchText
from DAGFlow.Operator.TextOperator import TextOperator
from DAGFlow.DAG import DAG
from bson.objectid import ObjectId

def main():
    # 只匹配上海期货交易相关的collection
    news_data_obj = NewsData(collection_pattern='^shfe')
    # 获取'2018-03-08' - '2018-03-08'的新闻数据
    news_data = news_data_obj.get_from_mongodb(date_range=('2018-03-08', '2018-03-08'))

    key_news = []
    ptext_obj = SHFEFutureText()
    for collection, news_list in news_data.items():
        for news in news_list:
            # 数据加载，并将题目放入正文一起处理
            ptext_obj.load_content("%s %s" % (news['title'], news['content']))
            # 对文本进行预处理
            ptext_obj.text_preprocess(ptext_obj.collect_key_information)
            if ptext_obj.key_sentences:
                news_source = MatchText.match_news_source_by_collection_name(collection)
                # 数据打包准备存入数据库
                news_dict = ptext_obj.pack_data(title=news['title'],
                                                post_date=news['post_date'],
                                                _id=news['_id'],
                                                news_source=news_source,
                                                )
                news_data_obj.save_news_features_to_mongodb(news_dict)
                key_news.append(news_dict)

if __name__ == '__main__':
    main()
```
### DAG模式
```python
#coding=utf-8
from pprint import pprint
from data.dataAPI import NewsData
from WordsVector import WordVector
from models.TextProcess import SHFEText
from models.MatchText import MatchText
from DAGFlow.Operator.TextOperator import TextOperator
from DAGFlow.DAG import DAG

# 1. 定义流图中的处理函数
# 新闻数据载入入口
def load_news(data):
    data['text_obj'].load_content("%s %s" % (data['news_data']['title'], data['news_data']['content']))
    return data
# 分词处理
def cut_words(data):
    data['text_obj'].cut_words()
    return data
# 按词性过滤没用的词汇
def skip_by_POS(data):
    data['text_obj'].skip_by_POS()
    return data
# 去除停止词
def drop_stop_words(data):
    data['text_obj'].drop_stop_words()
    return data
# 捕获关键词，并记录关键的句子
def find_key_sentence(data):
    data['text_obj'].collect_key_information()
    return data
# 打印结果
def print_data(data):
    if not data['text_obj'].unworthy_news():
        pprint(data['text_obj'].pack_data(title=data['news_data']['title'],
                                            post_date=data['news_data']['post_date'],
                                            _id=data['news_data']['_id'],
                                            news_source=data['news_source']
        ))
    return 0
# 保存结果
def save_data(data):
    if not data['text_obj'].unworthy_news():
        new_data = data['text_obj'].pack_data(title=data['news_data']['title'],
                                            post_date=data['news_data']['post_date'],
                                            _id=data['news_data']['_id'],
                                            news_source=data['news_source']
        )
        data['news_data_obj'].save_news_features_to_mongodb(new_data)
    return 0

def main():

    # 2. 定义DAG图的层级关系

    dag = DAG("test1")

    t1 = TextOperator("load_news", dag, load_news)

    t2 = TextOperator("cut_words", dag, cut_words)

    t3 = TextOperator("skip_by_POS", dag, skip_by_POS)

    t4 = TextOperator("drop_stop_words", dag, drop_stop_words)

    t5 = TextOperator("find_key_sentence", dag, find_key_sentence)

    t6 = TextOperator("print_data", dag, print_data)

    t7 = TextOperator("save_data", dag, save_data)
    
                        # start input data
                        # |
    t1.set_upstream(t2) # t1    load_news
                        # |
    t2.set_upstream(t3) # t2    cut_words
                        # |
    t3.set_upstream(t4) # t3    skip_by_POS
                        # |
    t4.set_upstream(t5) # t4    drop_stop_words
                        # |
    t5.set_upstream(t7) # t5    find_key_sentence
                        # |
                        # t6/t6 print_data/save data
                        # |
                        # end

    # # get news data
    news_data_obj = NewsData(collection_pattern='^shfe')
    news_data = news_data_obj.get_from_mongodb(date_range=('2018-02-08', '2018-05-10'))

    # text_factory_obj = TextFactory()

    for collection, news_list in news_data.items():
        news_source = MatchText.match_news_source_by_collection_name(collection)
        for news in news_list:
            # text_obj = text_factory_obj.create_obj(news_source)
            text_obj = SHFEFutureText()
            input_data = {
                'collection': collection,
                'news_data': news,
                'text_obj': text_obj,
                'news_source': news_source,
                'news_data_obj': news_data_obj
            }
            # 3. 数据逐条输入
            t1.input_data(input_data, 'root')
            # 4. 执行DAG
            dag.run()

if __name__ == '__main__':
    main()
```


# ToDoList

### 20180513
- - [ ] 适配中国金融商品交易所，大连商品交易所
- - [x] 优化DAG模式的新闻处理流程
- - [x] 完善文档资料
- - [x] 优化预处理逻辑
- - [x] 增加提取期货品种的功能

### 20180504

- - [x] Text类分解 -> 多个不同新闻源的处理方法 （插件的形式）
- - [ ] 处理多个交易所的新闻来源（上海期货交易所，中国金融商品交易所，大连商品交易所），可以按这个来分解Text类
- - [x] 加入流处理模式，利用字典定义整个处理流程
- - [x] 加入数据库写入接口模块
- - [x] 建立数据库，与原始数据隔离
- - [x] 期货代码只需保存字母和数字的形式

### 20180427

- - [x] 以费用类型（手续费、交易限额）作为类型，对新闻进行分类
- - [x] 根据某个期货代码，获取近期相关的（费用变动）新闻
- - [x] 找出所有的期货代码和期货货物名称，制作字典方便分词
- - [x] 统一数据获取接口
- - [x]  分词和向量化模块解耦

